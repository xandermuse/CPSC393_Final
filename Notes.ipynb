{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Description | Data Preparation | Hyperparameter Tuning |\n",
    "|-------|-------------|-----------------|-----------------------|\n",
    "| LSTM | Recurrent neural network suitable for time series data | Normalize data, create time windows | Bayesian hyperparameter tuning |\n",
    "| GRU | Similar to LSTM for time series data | Normalize data, create time windows | Bayesian hyperparameter tuning |\n",
    "| Transformer | Neural network for sequence-to-sequence learning | Create input sequences, encode time info with positional encodings | Bayesian hyperparameter tuning |\n",
    "| ARIMA | Statistical model specifically designed for time series forecasting | Identify appropriate parameters (p, d, q) based on autocorrelation and partial autocorrelation plots | Grid search, AIC, BIC |\n",
    "| Prophet | Time series forecasting model designed for seasonal and holiday effects | Provide date and value columns | Cross-validation, grid search |\n",
    "| Ensemble | Combines predictions of multiple models for improved accuracy | Train individual models on time series data | Weighted average, majority voting, or other method |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "classDiagram\n",
    "    class DataCollector {\n",
    "        +collectData(tickers: List[str], start: str, end: str): DataFrame\n",
    "        +preprocessData(data: DataFrame): DataFrame\n",
    "    }\n",
    "    class ModelFactory {\n",
    "        +createModel(modelType: String): Model\n",
    "    }\n",
    "\n",
    "    class Model {\n",
    "        +build_model()\n",
    "        +train(X_train, y_train, epochs, batch_size, validation_split, patience)\n",
    "        +predict(X_test)\n",
    "    }\n",
    "    class LSTMModel {\n",
    "        +build_model()\n",
    "        +train(X_train, y_train, epochs, batch_size, validation_split, patience)\n",
    "        +predict(X_test)\n",
    "    }\n",
    "    class GRUModel {\n",
    "        +build_model()\n",
    "        +train(X_train, y_train, epochs, batch_size, validation_split, patience)\n",
    "        +predict(X_test)\n",
    "    }\n",
    "    class TransformerModel {\n",
    "        +build_model()\n",
    "        +train(X_train, y_train, epochs, batch_size, validation_split, patience)\n",
    "        +predict(X_test)\n",
    "    }\n",
    "    class ARIMAModel {\n",
    "        +build_model()\n",
    "        +train(X_train, y_train, epochs, batch_size, validation_split, patience)\n",
    "        +predict(X_test)\n",
    "    }\n",
    "    class ProphetModel {\n",
    "        +build_model()\n",
    "        +train(X_train, y_train, epochs, batch_size, validation_split, patience)\n",
    "        +predict(X_test)\n",
    "    }\n",
    "\n",
    "    class EnsembleModel {\n",
    "        +addModel(model: Model, weight: float): void\n",
    "        +predict(input: DataFrame): DataFrame\n",
    "    }\n",
    "    DataCollector --> Model\n",
    "    ModelFactory --> Model\n",
    "    EnsembleModel --> Model\n",
    "    Model --|> LSTMModel : Inheritance\n",
    "    Model --|> GRUModel : Inheritance\n",
    "    Model --|> TransformerModel : Inheritance\n",
    "    Model --|> ARIMAModel : Inheritance\n",
    "    Model --|> ProphetModel : Inheritance\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### status\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Data Collection and Preprocessing](#data-collection-and-preprocessing)\n",
    "2. [Model Preparation](#model-preparation)\n",
    "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "4. [Model Training](#model-training)\n",
    "5. [Ensemble Model Creation](#ensemble-model-creation)\n",
    "6. [Model Evaluation](#model-evaluation)\n",
    "7. [Model Deployment and Prediction](#model-deployment-and-prediction)\n",
    "8. [Documentation and Reporting](#documentation-and-reporting)\n",
    "\n",
    "## Data Collection and Preprocessing<a name=\"data-collection-and-preprocessing\"></a>\n",
    "\n",
    "1. Collect stock data for TSLA, AMD, and COST from Yahoo Finance.\n",
    "2. Perform data cleaning, handling missing values, and data transformations as needed.\n",
    "3. Normalize the data for better performance with neural network models.\n",
    "\n",
    "## Model Preparation<a name=\"model-preparation\"></a>\n",
    "\n",
    "1. Implement the `ModelFactory` class for creating instances of different models.\n",
    "2. Implement each model (LSTM, GRU, Transformer, ARIMA, Prophet) as a subclass of the `Model` class, with methods for training and prediction.\n",
    "\n",
    "## Hyperparameter Tuning<a name=\"hyperparameter-tuning\"></a>\n",
    "\n",
    "1. Perform Bayesian hyperparameter tuning for LSTM, GRU, and Transformer models.\n",
    "2. Use grid search, AIC, or BIC to find the optimal parameters for ARIMA.\n",
    "3. Use cross-validation and grid search to find the best hyperparameters for Prophet.\n",
    "\n",
    "## Model Training<a name=\"model-training\"></a>\n",
    "\n",
    "1. Split the data into training and validation sets.\n",
    "2. Train each individual model using the training data.\n",
    "3. Evaluate the performance of each model on the validation data.\n",
    "\n",
    "## Ensemble Model Creation<a name=\"ensemble-model-creation\"></a>\n",
    "\n",
    "1. Instantiate the `EnsembleModel` class.\n",
    "2. Add each trained model to the ensemble, along with a weight (which could be based on their validation performance).\n",
    "\n",
    "## Model Evaluation<a name=\"model-evaluation\"></a>\n",
    "\n",
    "1. Use the `EnsembleModel` to predict stock prices on the validation data.\n",
    "2. Evaluate the ensemble model's performance using appropriate metrics (e.g., mean squared error, mean absolute error).\n",
    "\n",
    "## Model Deployment and Prediction<a name=\"model-deployment-and-prediction\"></a>\n",
    "\n",
    "1. Retrain the ensemble model on the entire dataset.\n",
    "2. Deploy the model to a suitable environment (e.g., cloud server, local machine).\n",
    "3. Use the ensemble model to predict future stock prices for TSLA, AMD, and COST.\n",
    "\n",
    "## Documentation and Reporting<a name=\"documentation-and-reporting\"></a>\n",
    "\n",
    "1. Document each step of the process, including data preprocessing, model implementation, and hyperparameter tuning.\n",
    "2. Create visualizations to help illustrate the performance of individual models and the ensemble model.\n",
    "3. Prepare a report or presentation to showcase your findings and the performance of the ensemble model.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Library | Description |\n",
    "|---------|-------------|\n",
    "| Keras and TensorFlow | These libraries will be used to build, train, and evaluate the LSTM, GRU, and Transformer models. Keras is built on top of TensorFlow and provides a higher-level API for constructing neural networks. |\n",
    "| scikit-learn (sklearn) | This library can be used for data preprocessing, splitting data into training and validation sets, and evaluating model performance using metrics like mean squared error and mean absolute error. You can also use its GridSearchCV functionality for hyperparameter tuning of the ARIMA and Prophet models. |\n",
    "| Pandas | This library is essential for handling and manipulating data in the form of DataFrames. You'll use it for loading, cleaning, and transforming the stock data. |\n",
    "| NumPy | This library is used for numerical operations and working with arrays. It is often used alongside Pandas and other libraries for data manipulation. |\n",
    "| Matplotlib and Seaborn | These libraries will be useful for creating visualizations, such as line plots, bar plots, and heatmaps, to analyze and present the performance of your models. |\n",
    "| Prophet | This library, developed by Facebook, will be used for creating and tuning the Prophet time series forecasting model. |\n",
    "| scikit-optimize | This library is useful for hyperparameter optimization, including Bayesian optimization. You can use it in combination with scikit-learn for tuning the hyperparameters of the LSTM, GRU, and Transformer models. |\n",
    "| tqdm | This library provides progress bars for various loops, which can be helpful for tracking the progress of model training, especially for large datasets and complex models. |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "gantt\n",
    "    title Final Project Timeline\n",
    "    dateFormat  YYYY-MM-DD\n",
    "\n",
    "    section Data Collection and Preprocessing\n",
    "    Collect and preprocess data       :done,    dc1, 2023-04-12, 1d\n",
    "\n",
    "    section LSTM Model\n",
    "    Implement LSTM Model              :done,    lstm1, after dc1, 3d\n",
    "\n",
    "    section GRU Model\n",
    "    Implement GRU Model               :active,  gru1, after lstm1, 3d\n",
    "\n",
    "    section Transformer Model\n",
    "    Implement Transformer Model       :         trans1, after gru1, 3d\n",
    "\n",
    "    section ARIMA Model\n",
    "    Implement ARIMA Model             :         arima1, after trans1, 3d\n",
    "\n",
    "    section Prophet Model\n",
    "    Implement Prophet Model           :         prophet1, after arima1, 3d\n",
    "\n",
    "    section Model Factory\n",
    "    Implement Model Factory           :         mf1, after prophet1, 1d\n",
    "\n",
    "    section Ensemble Model\n",
    "    Implement Ensemble Model          :         em1, after mf1, 6d\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
